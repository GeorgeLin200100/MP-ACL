Python 3.12.5
sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : 
trainer_spec {
  input: ./TinyStories-valid.txt
  input_format: 
  model_prefix: TinyStories-valid
  model_type: UNIGRAM
  vocab_size: 4096
  self_test_sample_size: 0
  character_coverage: 0.9995
  input_sentence_size: 1000000
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 0
  pretokenization_delimiter: 
  treat_whitespace_as_suffix: 0
  allow_whitespace_only_pieces: 0
  required_chars: 
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  seed_sentencepieces_file: 
  hard_vocab_limit: 1
  use_all_vocab: 0
  unk_id: 1
  bos_id: 2
  eos_id: 3
  pad_id: 0
  unk_piece: <unk>
  bos_piece: <s>
  eos_piece: </s>
  pad_piece: <pad>
  unk_surface:  ⁇ 
  enable_differential_privacy: 0
  differential_privacy_noise_level: 0
  differential_privacy_clipping_threshold: 0
}
normalizer_spec {
  name: nmt_nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}
denormalizer_spec {}
trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(185) LOG(INFO) Loading corpus: ./TinyStories-valid.txt
trainer_interface.cc(409) LOG(INFO) Loaded all 135579 sentences
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>
trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>
trainer_interface.cc(430) LOG(INFO) Normalizing sentences...
trainer_interface.cc(539) LOG(INFO) all chars count=19400527
trainer_interface.cc(550) LOG(INFO) Done: 99.9525% characters are covered.
trainer_interface.cc(560) LOG(INFO) Alphabet size=58
trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999525
trainer_interface.cc(592) LOG(INFO) Done! preprocessed 134683 sentences.
unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=11364903
unigram_model_trainer.cc(312) LOG(INFO) Initialized 37076 seed sentencepieces
trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 134683
trainer_interface.cc(609) LOG(INFO) Done! 40665
unigram_model_trainer.cc(602) LOG(INFO) Using 40665 sentences for EM training
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=17251 obj=9.63425 num_tokens=82298 num_tokens/piece=4.77062
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=12102 obj=7.51037 num_tokens=82358 num_tokens/piece=6.80532
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=9076 obj=7.43779 num_tokens=86370 num_tokens/piece=9.51631
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9073 obj=7.43011 num_tokens=86369 num_tokens/piece=9.51934
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=6804 obj=7.45571 num_tokens=94696 num_tokens/piece=13.9177
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=6804 obj=7.44789 num_tokens=94695 num_tokens/piece=13.9175
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5103 obj=7.4985 num_tokens=104911 num_tokens/piece=20.5587
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=5103 obj=7.48947 num_tokens=104899 num_tokens/piece=20.5563
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=4505 obj=7.51989 num_tokens=109064 num_tokens/piece=24.2095
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4505 obj=7.51489 num_tokens=109062 num_tokens/piece=24.2091
trainer_interface.cc(687) LOG(INFO) Saving model: TinyStories-valid.model
trainer_interface.cc(699) LOG(INFO) Saving vocabs: TinyStories-valid.vocab
['▁<', '|', 'endo', 'f', 'text', '|', '>']
[37, 17, 39, 34, 40, 17, 38]
['▁P', 'r', 'o', 'j', 'e', 'c', 't', '▁for', '▁the', '▁art', 'if', 'ic', 'i', 'al', '▁in', 't', 'e', 'll', 'ig', 'en', 'ce', '▁class', '▁in', '▁F', 'u', 'd', 'an', '.']
torch.Size([512, 128]) torch.Size([512, 128])
The <|endoftext|> id is : 1
total params: 24,142,080
model size: 92.470MB
Training on cuda.
Epoch 1/1:
Step 1/9637 - LR:0.0000 - train_loss: 8.488Step 11/9637 - LR:0.0000 - train_loss: 7.250Step 21/9637 - LR:0.0000 - train_loss: 6.776Step 31/9637 - LR:0.0000 - train_loss: 6.362Step 41/9637 - LR:0.0000 - train_loss: 6.032Step 51/9637 - LR:0.0000 - train_loss: 5.711Step 61/9637 - LR:0.0000 - train_loss: 5.625Step 71/9637 - LR:0.0000 - train_loss: 5.390Step 81/9637 - LR:0.0000 - train_loss: 5.189Step 91/9637 - LR:0.0000 - train_loss: 5.065Step 101/9637 - LR:0.0000 - train_loss: 4.931Step 111/9637 - LR:0.0000 - train_loss: 4.759Step 121/9637 - LR:0.0000 - train_loss: 4.701Step 131/9637 - LR:0.0000 - train_loss: 4.574Step 141/9637 - LR:0.0000 - train_loss: 4.503Step 151/9637 - LR:0.0000 - train_loss: 4.409Step 161/9637 - LR:0.0000 - train_loss: 4.352Step 171/9637 - LR:0.0000 - train_loss: 4.236Step 181/9637 - LR:0.0000 - train_loss: 4.133Step 191/9637 - LR:0.0000 - train_loss: 4.106Step 201/9637 - LR:0.0000 - train_loss: 4.093Step 211/9637 - LR:0.0000 - train_loss: 4.049Step 221/9637 - LR:0.0000 - train_loss: 3.962Step 231/9637 - LR:0.0000 - train_loss: 3.941Step 241/9637 - LR:0.0000 - train_loss: 3.892Step 251/9637 - LR:0.0000 - train_loss: 3.805Step 261/9637 - LR:0.0000 - train_loss: 3.798Step 271/9637 - LR:0.0000 - train_loss: 3.793Step 281/9637 - LR:0.0000 - train_loss: 3.702Step 291/9637 - LR:0.0000 - train_loss: 3.668Step 301/9637 - LR:0.0000 - train_loss: 3.655Step 311/9637 - LR:0.0000 - train_loss: 3.665Step 321/9637 - LR:0.0000 - train_loss: 3.610Step 331/9637 - LR:0.0000 - train_loss: 3.572Step 341/9637 - LR:0.0000 - train_loss: 3.550Step 351/9637 - LR:0.0000 - train_loss: 3.661Step 361/9637 - LR:0.0000 - train_loss: 3.586Step 371/9637 - LR:0.0000 - train_loss: 3.460Step 381/9637 - LR:0.0000 - train_loss: 3.482Step 391/9637 - LR:0.0000 - train_loss: 3.400Step 401/9637 - LR:0.0000 - train_loss: 3.425Step 411/9637 - LR:0.0000 - train_loss: 3.441Step 421/9637 - LR:0.0000 - train_loss: 3.427Step 431/9637 - LR:0.0000 - train_loss: 3.359Step 441/9637 - LR:0.0000 - train_loss: 3.342Step 451/9637 - LR:0.0000 - train_loss: 3.300Step 461/9637 - LR:0.0000 - train_loss: 3.291Step 471/9637 - LR:0.0000 - train_loss: 3.287Step 481/9637 - LR:0.0000 - train_loss: 3.261Step 491/9637 - LR:0.0000 - train_loss: 3.280Step 501/9637 - LR:0.0000 - train_loss: 3.222Step 511/9637 - LR:0.0000 - train_loss: 3.231Step 521/9637 - LR:0.0000 - train_loss: 3.199Step 531/9637 - LR:0.0000 - train_loss: 3.218Step 541/9637 - LR:0.0000 - train_loss: 3.168Step 551/9637 - LR:0.0000 - train_loss: 3.152Step 561/9637 - LR:0.0000 - train_loss: 3.130Step 571/9637 - LR:0.0000 - train_loss: 3.136Step 581/9637 - LR:0.0000 - train_loss: 3.111Step 591/9637 - LR:0.0000 - train_loss: 3.134Step 601/9637 - LR:0.0000 - train_loss: 3.096Step 611/9637 - LR:0.0000 - train_loss: 3.135Step 621/9637 - LR:0.0000 - train_loss: 3.050Step 631/9637 - LR:0.0000 - train_loss: 3.096Step 641/9637 - LR:0.0000 - train_loss: 3.032Step 651/9637 - LR:0.0000 - train_loss: 3.049Step 661/9637 - LR:0.0000 - train_loss: 3.006Step 671/9637 - LR:0.0000 - train_loss: 2.982Step 681/9637 - LR:0.0000 - train_loss: 3.001Step 691/9637 - LR:0.0000 - train_loss: 2.955Step 701/9637 - LR:0.0000 - train_loss: 2.957Step 711/9637 - LR:0.0000 - train_loss: 2.957Step 721/9637 - LR:0.0000 - train_loss: 2.957Step 731/9637 - LR:0.0000 - train_loss: 2.911Step 741/9637 - LR:0.0000 - train_loss: 2.931Step 751/9637 - LR:0.0000 - train_loss: 2.910Step 761/9637 - LR:0.0000 - train_loss: 2.934Step 771/9637 - LR:0.0000 - train_loss: 2.904Step 781/9637 - LR:0.0000 - train_loss: 2.904Step 791/9637 - LR:0.0000 - train_loss: 2.880Step 801/9637 - LR:0.0000 - train_loss: 2.872Step 811/9637 - LR:0.0000 - train_loss: 2.881Step 821/9637 - LR:0.0000 - train_loss: 2.873Step 831/9637 - LR:0.0000 - train_loss: 2.840Step 841/9637 - LR:0.0000 - train_loss: 2.807Step 851/9637 - LR:0.0000 - train_loss: 2.830Step 861/9637 - LR:0.0000 - train_loss: 2.808Step 871/9637 - LR:0.0000 - train_loss: 2.797Step 881/9637 - LR:0.0000 - train_loss: 2.818Step 891/9637 - LR:0.0000 - train_loss: 2.800Step 901/9637 - LR:0.0000 - train_loss: 2.818Step 911/9637 - LR:0.0000 - train_loss: 2.759Step 921/9637 - LR:0.0000 - train_loss: 2.778Step 931/9637 - LR:0.0000 - train_loss: 2.754Step 941/9637 - LR:0.0000 - train_loss: 2.759Step 951/9637 - LR:0.0000 - train_loss: 2.746Step 961/9637 - LR:0.0000 - train_loss: 2.724Step 971/9637 - LR:0.0000 - train_loss: 2.748Step 981/9637 - LR:0.0000 - train_loss: 2.772Step 991/9637 - LR:0.0000 - train_loss: 2.713Step 1001/9637 - LR:0.0000 - train_loss: 2.709Step 1011/9637 - LR:0.0000 - train_loss: 2.684Step 1021/9637 - LR:0.0000 - train_loss: 2.658Step 1031/9637 - LR:0.0000 - train_loss: 2.704Step 1041/9637 - LR:0.0000 - train_loss: 2.685Step 1051/9637 - LR:0.0000 - train_loss: 2.671Step 1061/9637 - LR:0.0000 - train_loss: 2.650Step 1071/9637 - LR:0.0000 - train_loss: 2.665Step 1081/9637 - LR:0.0000 - train_loss: 2.695Step 1091/9637 - LR:0.0000 - train_loss: 2.644Step 1101/9637 - LR:0.0000 - train_loss: 2.648Step 1111/9637 - LR:0.0000 - train_loss: 2.664Step 1121/9637 - LR:0.0000 - train_loss: 2.627Step 1131/9637 - LR:0.0000 - train_loss: 2.605Step 1141/9637 - LR:0.0000 - train_loss: 2.589Step 1151/9637 - LR:0.0000 - train_loss: 2.611Step 1161/9637 - LR:0.0000 - train_loss: 2.621Step 1171/9637 - LR:0.0000 - train_loss: 2.590Step 1181/9637 - LR:0.0000 - train_loss: 2.584Step 1191/9637 - LR:0.0000 - train_loss: 2.598Step 1201/9637 - LR:0.0000 - train_loss: 2.576Step 1211/9637 - LR:0.0000 - train_loss: 2.569Step 1221/9637 - LR:0.0000 - train_loss: 2.588Step 1231/9637 - LR:0.0000 - train_loss: 2.555Step 1241/9637 - LR:0.0000 - train_loss: 2.554Step 1251/9637 - LR:0.0000 - train_loss: 2.573Step 1261/9637 - LR:0.0000 - train_loss: 2.569Step 1271/9637 - LR:0.0000 - train_loss: 2.545Step 1281/9637 - LR:0.0000 - train_loss: 2.566Step 1291/9637 - LR:0.0000 - train_loss: 2.535Step 1301/9637 - LR:0.0000 - train_loss: 2.563Step 1311/9637 - LR:0.0000 - train_loss: 2.523Step 1321/9637 - LR:0.0000 - train_loss: 2.497Step 1331/9637 - LR:0.0000 - train_loss: 2.494Step 1341/9637 - LR:0.0000 - train_loss: 2.498Step 1351/9637 - LR:0.0000 - train_loss: 2.518Step 1361/9637 - LR:0.0000 - train_loss: 2.518Step 1371/9637 - LR:0.0000 - train_loss: 2.489Step 1381/9637 - LR:0.0000 - train_loss: 2.488Step 1391/9637 - LR:0.0000 - train_loss: 2.480Step 1401/9637 - LR:0.0000 - train_loss: 2.495Step 1411/9637 - LR:0.0000 - train_loss: 2.495Step 1421/9637 - LR:0.0000 - train_loss: 2.487Step 1431/9637 - LR:0.0000 - train_loss: 2.509Step 1441/9637 - LR:0.0000 - train_loss: 2.492Step 1451/9637 - LR:0.0000 - train_loss: 2.456Step 1461/9637 - LR:0.0000 - train_loss: 2.444Step 1471/9637 - LR:0.0000 - train_loss: 2.461Step 1481/9637 - LR:0.0000 - train_loss: 2.455Step 1491/9637 - LR:0.0000 - train_loss: 2.422Step 1501/9637 - LR:0.0000 - train_loss: 2.443Step 1511/9637 - LR:0.0000 - train_loss: 2.424Step 1521/9637 - LR:0.0000 - train_loss: 2.419Step 1531/9637 - LR:0.0000 - train_loss: 2.420Step 1541/9637 - LR:0.0000 - train_loss: 2.444Step 1551/9637 - LR:0.0000 - train_loss: 2.376Step 1561/9637 - LR:0.0000 - train_loss: 2.382Step 1571/9637 - LR:0.0000 - train_loss: 2.391Step 1581/9637 - LR:0.0000 - train_loss: 2.394Step 1591/9637 - LR:0.0000 - train_loss: 2.409Step 1601/9637 - LR:0.0000 - train_loss: 2.410Step 1611/9637 - LR:0.0000 - train_loss: 2.424Step 1621/9637 - LR:0.0000 - train_loss: 2.344Step 1631/9637 - LR:0.0000 - train_loss: 2.401Step 1641/9637 - LR:0.0000 - train_loss: 2.360Step 1651/9637 - LR:0.0000 - train_loss: 2.377Step 1661/9637 - LR:0.0000 - train_loss: 2.370Step 1671/9637 - LR:0.0000 - train_loss: 2.383Step 1681/9637 - LR:0.0000 - train_loss: 2.383Step 1691/9637 - LR:0.0000 - train_loss: 2.372Step 1701/9637 - LR:0.0000 - train_loss: 2.377Step 1711/9637 - LR:0.0000 - train_loss: 2.356Step 1721/9637 - LR:0.0000 - train_loss: 2.334Step 1731/9637 - LR:0.0000 - train_loss: 2.380Step 1741/9637 - LR:0.0000 - train_loss: 2.339Step 1751/9637 - LR:0.0000 - train_loss: 2.355Step 1761/9637 - LR:0.0000 - train_loss: 2.346Step 1771/9637 - LR:0.0000 - train_loss: 2.332Step 1781/9637 - LR:0.0000 - train_loss: 2.330Step 1791/9637 - LR:0.0000 - train_loss: 2.315Step 1801/9637 - LR:0.0000 - train_loss: 2.322Step 1811/9637 - LR:0.0000 - train_loss: 2.355Step 1821/9637 - LR:0.0000 - train_loss: 2.303Step 1831/9637 - LR:0.0000 - train_loss: 2.282Step 1841/9637 - LR:0.0000 - train_loss: 2.322Step 1851/9637 - LR:0.0000 - train_loss: 2.337Step 1861/9637 - LR:0.0000 - train_loss: 2.281Step 1871/9637 - LR:0.0000 - train_loss: 2.318Step 1881/9637 - LR:0.0000 - train_loss: 2.310Step 1891/9637 - LR:0.0000 - train_loss: 2.328Step 1901/9637 - LR:0.0000 - train_loss: 2.306Step 1911/9637 - LR:0.0000 - train_loss: 2.293Step 1921/9637 - LR:0.0000 - train_loss: 2.286Step 1931/9637 - LR:0.0000 - train_loss: 2.261Step 1941/9637 - LR:0.0000 - train_loss: 2.268Step 1951/9637 - LR:0.0000 - train_loss: 2.297Step 1961/9637 - LR:0.0000 - train_loss: 2.248Step 1971/9637 - LR:0.0000 - train_loss: 2.267Step 1981/9637 - LR:0.0000 - train_loss: 2.300Step 1991/9637 - LR:0.0000 - train_loss: 2.267Step 2001/9637 - LR:0.0000 - train_loss: 2.270Step 2011/9637 - LR:0.0000 - train_loss: 2.247Step 2021/9637 - LR:0.0000 - train_loss: 2.276Step 2031/9637 - LR:0.0000 - train_loss: 2.244Step 2041/9637 - LR:0.0000 - train_loss: 2.279Step 2051/9637 - LR:0.0000 - train_loss: 2.260Step 2061/9637 - LR:0.0000 - train_loss: 2.251Step 2071/9637 - LR:0.0000 - train_loss: 2.238Step 2081/9637 - LR:0.0000 - train_loss: 2.268Step 2091/9637 - LR:0.0000 - train_loss: 2.266Step 2101/9637 - LR:0.0000 - train_loss: 2.228Step 2111/9637 - LR:0.0000 - train_loss: 2.266Step 2121/9637 - LR:0.0000 - train_loss: 2.256Step 2131/9637 - LR:0.0000 - train_loss: 2.214Step 2141/9637 - LR:0.0000 - train_loss: 2.215Step 2151/9637 - LR:0.0000 - train_loss: 2.210Step 2161/9637 - LR:0.0000 - train_loss: 2.230Step 2171/9637 - LR:0.0000 - train_loss: 2.195Step 2181/9637 - LR:0.0000 - train_loss: 2.229Step 2191/9637 - LR:0.0000 - train_loss: 2.200Step 2201/9637 - LR:0.0000 - train_loss: 2.245Step 2211/9637 - LR:0.0000 - train_loss: 2.198Step 2221/9637 - LR:0.0000 - train_loss: 2.199Step 2231/9637 - LR:0.0000 - train_loss: 2.206Step 2241/9637 - LR:0.0000 - train_loss: 2.223Step 2251/9637 - LR:0.0000 - train_loss: 2.185Step 2261/9637 - LR:0.0000 - train_loss: 2.213Step 2271/9637 - LR:0.0000 - train_loss: 2.180Step 2281/9637 - LR:0.0000 - train_loss: 2.164Step 2291/9637 - LR:0.0000 - train_loss: 2.219Step 2301/9637 - LR:0.0000 - train_loss: 2.235Step 2311/9637 - LR:0.0000 - train_loss: 2.172Step 2321/9637 - LR:0.0000 - train_loss: 2.164Step 2331/9637 - LR:0.0000 - train_loss: 2.169Step 2341/9637 - LR:0.0000 - train_loss: 2.206Step 2351/9637 - LR:0.0000 - train_loss: 2.209Step 2361/9637 - LR:0.0000 - train_loss: 2.170Step 2371/9637 - LR:0.0000 - train_loss: 2.171Step 2381/9637 - LR:0.0000 - train_loss: 2.134Step 2391/9637 - LR:0.0000 - train_loss: 2.166Step 2401/9637 - LR:0.0000 - train_loss: 2.166Step 2411/9637 - LR:0.0000 - train_loss: 2.180Step 2421/9637 - LR:0.0000 - train_loss: 2.148Step 2431/9637 - LR:0.0000 - train_loss: 2.165Step 2441/9637 - LR:0.0000 - train_loss: 2.152Step 2451/9637 - LR:0.0000 - train_loss: 2.138Step 2461/9637 - LR:0.0000 - train_loss: 2.143Step 2471/9637 - LR:0.0000 - train_loss: 2.130Step 2481/9637 - LR:0.0000 - train_loss: 2.160Step 2491/9637 - LR:0.0000 - train_loss: 2.166Step 2501/9637 - LR:0.0000 - train_loss: 2.159Step 2511/9637 - LR:0.0000 - train_loss: 2.159Step 2521/9637 - LR:0.0000 - train_loss: 2.140Step 2531/9637 - LR:0.0000 - train_loss: 2.139Step 2541/9637 - LR:0.0000 - train_loss: 2.107Step 2551/9637 - LR:0.0000 - train_loss: 2.147Step 2561/9637 - LR:0.0000 - train_loss: 2.110Step 2571/9637 - LR:0.0000 - train_loss: 2.177Step 2581/9637 - LR:0.0000 - train_loss: 2.132Step 2591/9637 - LR:0.0000 - train_loss: 2.105Step 2601/9637 - LR:0.0000 - train_loss: 2.164Step 2611/9637 - LR:0.0000 - train_loss: 2.138Step 2621/9637 - LR:0.0000 - train_loss: 2.149Step 2631/9637 - LR:0.0000 - train_loss: 2.138Step 2641/9637 - LR:0.0000 - train_loss: 2.122Step 2651/9637 - LR:0.0000 - train_loss: 2.139Step 2661/9637 - LR:0.0000 - train_loss: 2.108Step 2671/9637 - LR:0.0000 - train_loss: 2.137Step 2681/9637 - LR:0.0000 - train_loss: 2.096Step 2691/9637 - LR:0.0000 - train_loss: 2.063Step 2701/9637 - LR:0.0000 - train_loss: 2.114Step 2711/9637 - LR:0.0000 - train_loss: 2.074Step 2721/9637 - LR:0.0000 - train_loss: 2.101Step 2731/9637 - LR:0.0000 - train_loss: 2.095Step 2741/9637 - LR:0.0000 - train_loss: 2.128Step 2751/9637 - LR:0.0000 - train_loss: 2.098Step 2761/9637 - LR:0.0000 - train_loss: 2.052Step 2771/9637 - LR:0.0000 - train_loss: 2.090Step 2781/9637 - LR:0.0000 - train_loss: 2.093Step 2791/9637 - LR:0.0000 - train_loss: 2.099Step 2801/9637 - LR:0.0000 - train_loss: 2.083Step 2811/9637 - LR:0.0000 - train_loss: 2.090Step 2821/9637 - LR:0.0000 - train_loss: 2.106Step 2831/9637 - LR:0.0000 - train_loss: 2.061Step 2841/9637 - LR:0.0000 - train_loss: 2.075Step 2851/9637 - LR:0.0000 - train_loss: 2.072Step 2861/9637 - LR:0.0000 - train_loss: 2.063Step 2871/9637 - LR:0.0000 - train_loss: 2.076Step 2881/9637 - LR:0.0000 - train_loss: 2.063Step 2891/9637 - LR:0.0000 - train_loss: 2.089Step 2901/9637 - LR:0.0000 - train_loss: 2.064Step 2911/9637 - LR:0.0000 - train_loss: 2.056Step 2921/9637 - LR:0.0000 - train_loss: 2.041Step 2931/9637 - LR:0.0000 - train_loss: 2.069Step 2941/9637 - LR:0.0000 - train_loss: 2.060Step 2951/9637 - LR:0.0000 - train_loss: 2.032Step 2961/9637 - LR:0.0000 - train_loss: 2.034Step 2971/9637 - LR:0.0000 - train_loss: 2.081Step 2981/9637 - LR:0.0000 - train_loss: 2.037Step 2991/9637 - LR:0.0000 - train_loss: 2.059Step 3001/9637 - LR:0.0000 - train_loss: 2.048Step 3011/9637 - LR:0.0000 - train_loss: 2.051Step 3021/9637 - LR:0.0000 - train_loss: 2.053Step 3031/9637 - LR:0.0000 - train_loss: 2.023Step 3041/9637 - LR:0.0000 - train_loss: 2.020Step 3051/9637 - LR:0.0000 - train_loss: 2.029Step 3061/9637 - LR:0.0000 - train_loss: 2.032Step 3071/9637 - LR:0.0000 - train_loss: 2.018Step 3081/9637 - LR:0.0000 - train_loss: 2.040Step 3091/9637 - LR:0.0000 - train_loss: 2.025Step 3101/9637 - LR:0.0000 - train_loss: 2.014Step 3111/9637 - LR:0.0000 - train_loss: 1.998Step 3121/9637 - LR:0.0000 - train_loss: 2.004Step 3131/9637 - LR:0.0000 - train_loss: 2.004Step 3141/9637 - LR:0.0000 - train_loss: 2.039Step 3151/9637 - LR:0.0000 - train_loss: 2.026Step 3161/9637 - LR:0.0000 - train_loss: 2.022Step 3171/9637 - LR:0.0000 - train_loss: 2.045Step 3181/9637 - LR:0.0000 - train_loss: 2.003Step 3191/9637 - LR:0.0000 - train_loss: 2.014Step 3201/9637 - LR:0.0000 - train_loss: 2.018Step 3211/9637 - LR:0.0000 - train_loss: 2.030Step 3221/9637 - LR:0.0000 - train_loss: 2.055Step 3231/9637 - LR:0.0000 - train_loss: 1.995Step 3241/9637 - LR:0.0000 - train_loss: 2.051Step 3251/9637 - LR:0.0000 - train_loss: 1.978Step 3261/9637 - LR:0.0000 - train_loss: 2.052Step 3271/9637 - LR:0.0000 - train_loss: 2.006Step 3281/9637 - LR:0.0000 - train_loss: 2.000Step 3291/9637 - LR:0.0000 - train_loss: 1.993Step 3301/9637 - LR:0.0000 - train_loss: 2.003Step 3311/9637 - LR:0.0000 - train_loss: 1.979Step 3321/9637 - LR:0.0000 - train_loss: 1.996Step 3331/9637 - LR:0.0000 - train_loss: 1.986Step 3341/9637 - LR:0.0000 - train_loss: 1.959Step 3351/9637 - LR:0.0000 - train_loss: 1.972Step 3361/9637 - LR:0.0000 - train_loss: 1.967Step 3371/9637 - LR:0.0000 - train_loss: 2.023Step 3381/9637 - LR:0.0000 - train_loss: 2.012Step 3391/9637 - LR:0.0000 - train_loss: 1.976Step 3401/9637 - LR:0.0000 - train_loss: 1.981Step 3411/9637 - LR:0.0000 - train_loss: 1.984Step 3421/9637 - LR:0.0000 - train_loss: 2.000Step 3431/9637 - LR:0.0000 - train_loss: 1.977Step 3441/9637 - LR:0.0000 - train_loss: 1.987Step 3451/9637 - LR:0.0000 - train_loss: 1.990Step 3461/9637 - LR:0.0000 - train_loss: 1.988Step 3471/9637 - LR:0.0000 - train_loss: 1.948Step 3481/9637 - LR:0.0000 - train_loss: 1.970Step 3491/9637 - LR:0.0000 - train_loss: 1.934Step 3501/9637 - LR:0.0000 - train_loss: 1.953Step 3511/9637 - LR:0.0000 - train_loss: 1.964Step 3521/9637 - LR:0.0000 - train_loss: 1.963Step 3531/9637 - LR:0.0000 - train_loss: 1.971Step 3541/9637 - LR:0.0000 - train_loss: 1.965Step 3551/9637 - LR:0.0000 - train_loss: 1.951Step 3561/9637 - LR:0.0000 - train_loss: 1.965Step 3571/9637 - LR:0.0000 - train_loss: 1.971Step 3581/9637 - LR:0.0000 - train_loss: 1.933Step 3591/9637 - LR:0.0000 - train_loss: 1.951Step 3601/9637 - LR:0.0000 - train_loss: 1.934Step 3611/9637 - LR:0.0000 - train_loss: 1.988Step 3621/9637 - LR:0.0000 - train_loss: 1.946Step 3631/9637 - LR:0.0000 - train_loss: 1.972Step 3641/9637 - LR:0.0000 - train_loss: 1.969Step 3651/9637 - LR:0.0000 - train_loss: 1.938Step 3661/9637 - LR:0.0000 - train_loss: 1.955Step 3671/9637 - LR:0.0000 - train_loss: 1.926Step 3681/9637 - LR:0.0000 - train_loss: 1.926Step 3691/9637 - LR:0.0000 - train_loss: 1.946Step 3701/9637 - LR:0.0000 - train_loss: 1.966Step 3711/9637 - LR:0.0000 - train_loss: 1.971Step 3721/9637 - LR:0.0000 - train_loss: 1.893Step 3731/9637 - LR:0.0000 - train_loss: 1.925Step 3741/9637 - LR:0.0000 - train_loss: 1.938Step 3751/9637 - LR:0.0000 - train_loss: 1.930Step 3761/9637 - LR:0.0000 - train_loss: 1.951Step 3771/9637 - LR:0.0000 - train_loss: 1.928Step 3781/9637 - LR:0.0000 - train_loss: 1.954Step 3791/9637 - LR:0.0000 - train_loss: 1.927Step 3801/9637 - LR:0.0000 - train_loss: 1.931Step 3811/9637 - LR:0.0000 - train_loss: 1.938Step 3821/9637 - LR:0.0000 - train_loss: 1.920Step 3831/9637 - LR:0.0000 - train_loss: 1.922Step 3841/9637 - LR:0.0000 - train_loss: 1.893Step 3851/9637 - LR:0.0000 - train_loss: 1.921Step 3861/9637 - LR:0.0000 - train_loss: 1.914Step 3871/9637 - LR:0.0000 - train_loss: 1.921Step 3881/9637 - LR:0.0000 - train_loss: 1.927Step 3891/9637 - LR:0.0000 - train_loss: 1.882Step 3901/9637 - LR:0.0000 - train_loss: 1.922Step 3911/9637 - LR:0.0000 - train_loss: 1.927Step 3921/9637 - LR:0.0000 - train_loss: 1.905Step 3931/9637 - LR:0.0000 - train_loss: 1.890Step 3941/9637 - LR:0.0000 - train_loss: 1.912Step 3951/9637 - LR:0.0000 - train_loss: 1.904Step 3961/9637 - LR:0.0000 - train_loss: 1.905Step 3971/9637 - LR:0.0000 - train_loss: 1.887Step 3981/9637 - LR:0.0000 - train_loss: 1.866Step 3991/9637 - LR:0.0000 - train_loss: 1.886Step 4001/9637 - LR:0.0000 - train_loss: 1.951Step 4011/9637 - LR:0.0000 - train_loss: 1.904Step 4021/9637 - LR:0.0000 - train_loss: 1.904Step 4031/9637 - LR:0.0000 - train_loss: 1.920Step 4041/9637 - LR:0.0000 - train_loss: 1.870Step 4051/9637 - LR:0.0000 - train_loss: 1.895Step 4061/9637 - LR:0.0000 - train_loss: 1.890Step 4071/9637 - LR:0.0000 - train_loss: 1.911Step 4081/9637 - LR:0.0000 - train_loss: 1.903Step 4091/9637 - LR:0.0000 - train_loss: 1.875Step 4101/9637 - LR:0.0000 - train_loss: 1.894Step 4111/9637 - LR:0.0000 - train_loss: 1.886Step 4121/9637 - LR:0.0000 - train_loss: 1.902Step 4131/9637 - LR:0.0000 - train_loss: 1.859Step 4141/9637 - LR:0.0000 - train_loss: 1.877Step 4151/9637 - LR:0.0000 - train_loss: 1.876Step 4161/9637 - LR:0.0000 - train_loss: 1.875Step 4171/9637 - LR:0.0000 - train_loss: 1.868Step 4181/9637 - LR:0.0000 - train_loss: 1.867Step 4191/9637 - LR:0.0000 - train_loss: 1.896Step 4201/9637 - LR:0.0000 - train_loss: 1.872Step 4211/9637 - LR:0.0000 - train_loss: 1.846Step 4221/9637 - LR:0.0000 - train_loss: 1.859Step 4231/9637 - LR:0.0000 - train_loss: 1.883Step 4241/9637 - LR:0.0000 - train_loss: 1.881Step 4251/9637 - LR:0.0000 - train_loss: 1.850Step 4261/9637 - LR:0.0000 - train_loss: 1.892Step 4271/9637 - LR:0.0000 - train_loss: 1.843Step 4281/9637 - LR:0.0000 - train_loss: 1.855Step 4291/9637 - LR:0.0000 - train_loss: 1.874Step 4301/9637 - LR:0.0000 - train_loss: 1.874Step 4311/9637 - LR:0.0000 - train_loss: 1.850Step 4321/9637 - LR:0.0000 - train_loss: 1.846Step 4331/9637 - LR:0.0000 - train_loss: 1.848Step 4341/9637 - LR:0.0000 - train_loss: 1.863Step 4351/9637 - LR:0.0000 - train_loss: 1.853Step 4361/9637 - LR:0.0000 - train_loss: 1.850Step 4371/9637 - LR:0.0000 - train_loss: 1.832Step 4381/9637 - LR:0.0000 - train_loss: 1.870Step 4391/9637 - LR:0.0000 - train_loss: 1.843Step 4401/9637 - LR:0.0000 - train_loss: 1.863Step 4411/9637 - LR:0.0000 - train_loss: 1.850Step 4421/9637 - LR:0.0000 - train_loss: 1.822Step 4431/9637 - LR:0.0000 - train_loss: 1.850Step 4441/9637 - LR:0.0000 - train_loss: 1.830Step 4451/9637 - LR:0.0000 - train_loss: 1.849Step 4461/9637 - LR:0.0000 - train_loss: 1.841Step 4471/9637 - LR:0.0000 - train_loss: 1.845Step 4481/9637 - LR:0.0000 - train_loss: 1.817Step 4491/9637 - LR:0.0000 - train_loss: 1.871Step 4501/9637 - LR:0.0000 - train_loss: 1.842Step 4511/9637 - LR:0.0000 - train_loss: 1.821Step 4521/9637 - LR:0.0000 - train_loss: 1.860Step 4531/9637 - LR:0.0000 - train_loss: 1.824Step 4541/9637 - LR:0.0000 - train_loss: 1.803Step 4551/9637 - LR:0.0000 - train_loss: 1.827Step 4561/9637 - LR:0.0000 - train_loss: 1.847Step 4571/9637 - LR:0.0000 - train_loss: 1.825Step 4581/9637 - LR:0.0000 - train_loss: 1.823Step 4591/9637 - LR:0.0000 - train_loss: 1.836Step 4601/9637 - LR:0.0000 - train_loss: 1.827Step 4611/9637 - LR:0.0000 - train_loss: 1.815Step 4621/9637 - LR:0.0000 - train_loss: 1.809Step 4631/9637 - LR:0.0000 - train_loss: 1.823Step 4641/9637 - LR:0.0000 - train_loss: 1.829Step 4651/9637 - LR:0.0000 - train_loss: 1.827Step 4661/9637 - LR:0.0000 - train_loss: 1.845Step 4671/9637 - LR:0.0000 - train_loss: 1.809Step 4681/9637 - LR:0.0000 - train_loss: 1.835Step 4691/9637 - LR:0.0000 - train_loss: 1.823Step 4701/9637 - LR:0.0000 - train_loss: 1.810Step 4711/9637 - LR:0.0000 - train_loss: 1.842Step 4721/9637 - LR:0.0000 - train_loss: 1.789Step 4731/9637 - LR:0.0000 - train_loss: 1.808Step 4741/9637 - LR:0.0000 - train_loss: 1.783Step 4751/9637 - LR:0.0000 - train_loss: 1.808Step 4761/9637 - LR:0.0000 - train_loss: 1.779Step 4771/9637 - LR:0.0000 - train_loss: 1.794Step 4781/9637 - LR:0.0000 - train_loss: 1.809Step 4791/9637 - LR:0.0000 - train_loss: 1.806Step 4801/9637 - LR:0.0000 - train_loss: 1.789Step 4811/9637 - LR:0.0000 - train_loss: 1.804Step 4821/9637 - LR:0.0000 - train_loss: 1.801Step 4831/9637 - LR:0.0000 - train_loss: 1.799Step 4841/9637 - LR:0.0000 - train_loss: 1.783Step 4851/9637 - LR:0.0000 - train_loss: 1.800Step 4861/9637 - LR:0.0000 - train_loss: 1.800Step 4871/9637 - LR:0.0000 - train_loss: 1.831Step 4881/9637 - LR:0.0000 - train_loss: 1.826Step 4891/9637 - LR:0.0000 - train_loss: 1.815Step 4901/9637 - LR:0.0000 - train_loss: 1.815Step 4911/9637 - LR:0.0000 - train_loss: 1.778Step 4921/9637 - LR:0.0000 - train_loss: 1.802Step 4931/9637 - LR:0.0000 - train_loss: 1.799Step 4941/9637 - LR:0.0000 - train_loss: 1.767Step 4951/9637 - LR:0.0000 - train_loss: 1.782Step 4961/9637 - LR:0.0000 - train_loss: 1.776Step 4971/9637 - LR:0.0000 - train_loss: 1.775Step 4981/9637 - LR:0.0000 - train_loss: 1.786Step 4991/9637 - LR:0.0000 - train_loss: 1.749Step 5001/9637 - LR:0.0000 - train_loss: 1.785Step 5011/9637 - LR:0.0000 - train_loss: 1.766Step 5021/9637 - LR:0.0000 - train_loss: 1.806Step 5031/9637 - LR:0.0000 - train_loss: 1.789Step 5041/9637 - LR:0.0000 - train_loss: 1.799Step 5051/9637 - LR:0.0000 - train_loss: 1.761Step 5061/9637 - LR:0.0000 - train_loss: 1.759Step 5071/9637 - LR:0.0000 - train_loss: 1.783Step 5081/9637 - LR:0.0000 - train_loss: 1.769Step 5091/9637 - LR:0.0000 - train_loss: 1.780Step 5101/9637 - LR:0.0000 - train_loss: 1.781Step 5111/9637 - LR:0.0000 - train_loss: 1.762Step 5121/9637 - LR:0.0000 - train_loss: 1.804Step 5131/9637 - LR:0.0000 - train_loss: 1.758Step 5141/9637 - LR:0.0000 - train_loss: 1.777Step 5151/9637 - LR:0.0000 - train_loss: 1.777Step 5161/9637 - LR:0.0000 - train_loss: 1.747Step 5171/9637 - LR:0.0000 - train_loss: 1.743Step 5181/9637 - LR:0.0000 - train_loss: 1.779Step 5191/9637 - LR:0.0000 - train_loss: 1.754Step 5201/9637 - LR:0.0000 - train_loss: 1.777Step 5211/9637 - LR:0.0000 - train_loss: 1.755Step 5221/9637 - LR:0.0000 - train_loss: 1.748Step 5231/9637 - LR:0.0000 - train_loss: 1.812Step 5241/9637 - LR:0.0000 - train_loss: 1.754Step 5251/9637 - LR:0.0000 - train_loss: 1.752Step 5261/9637 - LR:0.0000 - train_loss: 1.769Step 5271/9637 - LR:0.0000 - train_loss: 1.738Step 5281/9637 - LR:0.0000 - train_loss: 1.742Step 5291/9637 - LR:0.0000 - train_loss: 1.743Step 5301/9637 - LR:0.0000 - train_loss: 1.735Step 5311/9637 - LR:0.0000 - train_loss: 1.758Step 5321/9637 - LR:0.0000 - train_loss: 1.746Step 5331/9637 - LR:0.0000 - train_loss: 1.767Step 5341/9637 - LR:0.0000 - train_loss: 1.741Step 5351/9637 - LR:0.0000 - train_loss: 1.755Step 5361/9637 - LR:0.0000 - train_loss: 1.736Step 5371/9637 - LR:0.0000 - train_loss: 1.793Step 5381/9637 - LR:0.0000 - train_loss: 1.733Step 5391/9637 - LR:0.0000 - train_loss: 1.727Step 5401/9637 - LR:0.0000 - train_loss: 1.752Step 5411/9637 - LR:0.0000 - train_loss: 1.740Step 5421/9637 - LR:0.0000 - train_loss: 1.732Step 5431/9637 - LR:0.0000 - train_loss: 1.730Step 5441/9637 - LR:0.0000 - train_loss: 1.734Step 5451/9637 - LR:0.0000 - train_loss: 1.760Step 5461/9637 - LR:0.0000 - train_loss: 1.705Step 5471/9637 - LR:0.0000 - train_loss: 1.733Step 5481/9637 - LR:0.0000 - train_loss: 1.731Step 5491/9637 - LR:0.0000 - train_loss: 1.723Step 5501/9637 - LR:0.0000 - train_loss: 1.755Step 5511/9637 - LR:0.0000 - train_loss: 1.726Step 5521/9637 - LR:0.0000 - train_loss: 1.725Step 5531/9637 - LR:0.0000 - train_loss: 1.680Step 5541/9637 - LR:0.0000 - train_loss: 1.718Step 5551/9637 - LR:0.0000 - train_loss: 1.715Step 5561/9637 - LR:0.0000 - train_loss: 1.743Step 5571/9637 - LR:0.0000 - train_loss: 1.736Step 5581/9637 - LR:0.0000 - train_loss: 1.744Step 5591/9637 - LR:0.0000 - train_loss: 1.711Step 5601/9637 - LR:0.0000 - train_loss: 1.741Step 5611/9637 - LR:0.0000 - train_loss: 1.731Step 5621/9637 - LR:0.0000 - train_loss: 1.742Step 5631/9637 - LR:0.0000 - train_loss: 1.730Step 5641/9637 - LR:0.0000 - train_loss: 1.684Step 5651/9637 - LR:0.0000 - train_loss: 1.745Step 5661/9637 - LR:0.0000 - train_loss: 1.722Step 5671/9637 - LR:0.0000 - train_loss: 1.708Step 5681/9637 - LR:0.0000 - train_loss: 1.706Step 5691/9637 - LR:0.0000 - train_loss: 1.698Step 5701/9637 - LR:0.0000 - train_loss: 1.729Step 5711/9637 - LR:0.0000 - train_loss: 1.695Step 5721/9637 - LR:0.0000 - train_loss: 1.738Step 5731/9637 - LR:0.0000 - train_loss: 1.713Step 5741/9637 - LR:0.0000 - train_loss: 1.706Step 5751/9637 - LR:0.0000 - train_loss: 1.691Step 5761/9637 - LR:0.0000 - train_loss: 1.695Step 5771/9637 - LR:0.0000 - train_loss: 1.714Step 5781/9637 - LR:0.0000 - train_loss: 1.702Step 5791/9637 - LR:0.0000 - train_loss: 1.691Step 5801/9637 - LR:0.0000 - train_loss: 1.711Step 5811/9637 - LR:0.0000 - train_loss: 1.703Step 5821/9637 - LR:0.0000 - train_loss: 1.722Step 5831/9637 - LR:0.0000 - train_loss: 1.676Step 5841/9637 - LR:0.0000 - train_loss: 1.666Step 5851/9637 - LR:0.0000 - train_loss: 1.690Step 5861/9637 - LR:0.0000 - train_loss: 1.686Step 5871/9637 - LR:0.0000 - train_loss: 1.684Step 5881/9637 - LR:0.0000 - train_loss: 1.715Step 5891/9637 - LR:0.0000 - train_loss: 1.693Step 5901/9637 - LR:0.0000 - train_loss: 1.692Step 5911/9637 - LR:0.0000 - train_loss: 1.687Step 5921/9637 - LR:0.0000 - train_loss: 1.692Step 5931/9637 - LR:0.0000 - train_loss: 1.722Step 5941/9637 - LR:0.0000 - train_loss: 1.695Step 5951/9637 - LR:0.0000 - train_loss: 1.691Step 5961/9637 - LR:0.0000 - train_loss: 1.697Step 5971/9637 - LR:0.0000 - train_loss: 1.709Step 5981/9637 - LR:0.0000 - train_loss: 1.681Step 5991/9637 - LR:0.0000 - train_loss: 1.658Step 6001/9637 - LR:0.0000 - train_loss: 1.715Step 6011/9637 - LR:0.0000 - train_loss: 1.667Step 6021/9637 - LR:0.0000 - train_loss: 1.663Step 6031/9637 - LR:0.0000 - train_loss: 1.665Step 6041/9637 - LR:0.0000 - train_loss: 1.691Step 6051/9637 - LR:0.0000 - train_loss: 1.657Step 6061/9637 - LR:0.0000 - train_loss: 1.685Step 6071/9637 - LR:0.0000 - train_loss: 1.665Step 6081/9637 - LR:0.0000 - train_loss: 1.696Step 6091/9637 - LR:0.0000 - train_loss: 1.644Step 6101/9637 - LR:0.0000 - train_loss: 1.689Step 6111/9637 - LR:0.0000 - train_loss: 1.646Step 6121/9637 - LR:0.0000 - train_loss: 1.666Step 6131/9637 - LR:0.0000 - train_loss: 1.650Step 6141/9637 - LR:0.0000 - train_loss: 1.694Step 6151/9637 - LR:0.0000 - train_loss: 1.666Step 6161/9637 - LR:0.0000 - train_loss: 1.699Step 6171/9637 - LR:0.0000 - train_loss: 1.634Step 6181/9637 - LR:0.0000 - train_loss: 1.661Step 6191/9637 - LR:0.0000 - train_loss: 1.675Step 6201/9637 - LR:0.0000 - train_loss: 1.677Step 6211/9637 - LR:0.0000 - train_loss: 1.672Step 6221/9637 - LR:0.0000 - train_loss: 1.657Step 6231/9637 - LR:0.0000 - train_loss: 1.663Step 6241/9637 - LR:0.0000 - train_loss: 1.666Step 6251/9637 - LR:0.0000 - train_loss: 1.683Step 6261/9637 - LR:0.0000 - train_loss: 1.683Step 6271/9637 - LR:0.0000 - train_loss: 1.649Step 6281/9637 - LR:0.0000 - train_loss: 1.660Step 6291/9637 - LR:0.0000 - train_loss: 1.661Step 6301/9637 - LR:0.0000 - train_loss: 1.644Step 6311/9637 - LR:0.0000 - train_loss: 1.662Step 6321/9637 - LR:0.0000 - train_loss: 1.648Step 6331/9637 - LR:0.0000 - train_loss: 1.665Step 6341/9637 - LR:0.0000 - train_loss: 1.637Step 6351/9637 - LR:0.0000 - train_loss: 1.654Step 6361/9637 - LR:0.0000 - train_loss: 1.645Step 6371/9637 - LR:0.0000 - train_loss: 1.642Step 6381/9637 - LR:0.0000 - train_loss: 1.653Step 6391/9637 - LR:0.0000 - train_loss: 1.627Step 6401/9637 - LR:0.0000 - train_loss: 1.635Step 6411/9637 - LR:0.0000 - train_loss: 1.626Step 6421/9637 - LR:0.0000 - train_loss: 1.638Step 6431/9637 - LR:0.0000 - train_loss: 1.667Step 6441/9637 - LR:0.0000 - train_loss: 1.626Step 6451/9637 - LR:0.0000 - train_loss: 1.628Step 6461/9637 - LR:0.0000 - train_loss: 1.657Step 6471/9637 - LR:0.0000 - train_loss: 1.635Step 6481/9637 - LR:0.0000 - train_loss: 1.648Step 6491/9637 - LR:0.0000 - train_loss: 1.621Step 6501/9637 - LR:0.0000 - train_loss: 1.655Step 6511/9637 - LR:0.0000 - train_loss: 1.644Step 6521/9637 - LR:0.0000 - train_loss: 1.647Step 6531/9637 - LR:0.0000 - train_loss: 1.649Step 6541/9637 - LR:0.0000 - train_loss: 1.657Step 6551/9637 - LR:0.0000 - train_loss: 1.638Step 6561/9637 - LR:0.0000 - train_loss: 1.648Step 6571/9637 - LR:0.0000 - train_loss: 1.627Step 6581/9637 - LR:0.0000 - train_loss: 1.619Step 6591/9637 - LR:0.0000 - train_loss: 1.652Step 6601/9637 - LR:0.0000 - train_loss: 1.636Step 6611/9637 - LR:0.0000 - train_loss: 1.641Step 6621/9637 - LR:0.0000 - train_loss: 1.639Step 6631/9637 - LR:0.0000 - train_loss: 1.645Step 6641/9637 - LR:0.0000 - train_loss: 1.624Step 6651/9637 - LR:0.0000 - train_loss: 1.626Step 6661/9637 - LR:0.0000 - train_loss: 1.623Step 6671/9637 - LR:0.0000 - train_loss: 1.645Step 6681/9637 - LR:0.0000 - train_loss: 1.631Step 6691/9637 - LR:0.0000 - train_loss: 1.606Step 6701/9637 - LR:0.0000 - train_loss: 1.617Step 6711/9637 - LR:0.0000 - train_loss: 1.595Step 6721/9637 - LR:0.0000 - train_loss: 1.623Step 6731/9637 - LR:0.0000 - train_loss: 1.631Step 6741/9637 - LR:0.0000 - train_loss: 1.604Step 6751/9637 - LR:0.0000 - train_loss: 1.617Step 6761/9637 - LR:0.0000 - train_loss: 1.584Step 6771/9637 - LR:0.0000 - train_loss: 1.598Step 6781/9637 - LR:0.0000 - train_loss: 1.615Step 6791/9637 - LR:0.0000 - train_loss: 1.619Step 6801/9637 - LR:0.0000 - train_loss: 1.624Step 6811/9637 - LR:0.0000 - train_loss: 1.641Step 6821/9637 - LR:0.0000 - train_loss: 1.624Step 6831/9637 - LR:0.0000 - train_loss: 1.628Step 6841/9637 - LR:0.0000 - train_loss: 1.605Step 6851/9637 - LR:0.0000 - train_loss: 1.606Step 6861/9637 - LR:0.0000 - train_loss: 1.597Step 6871/9637 - LR:0.0000 - train_loss: 1.613Step 6881/9637 - LR:0.0000 - train_loss: 1.583Step 6891/9637 - LR:0.0000 - train_loss: 1.583Step 6901/9637 - LR:0.0000 - train_loss: 1.575Step 6911/9637 - LR:0.0000 - train_loss: 1.626Step 6921/9637 - LR:0.0000 - train_loss: 1.629Step 6931/9637 - LR:0.0000 - train_loss: 1.580Step 6941/9637 - LR:0.0000 - train_loss: 1.607Step 6951/9637 - LR:0.0000 - train_loss: 1.627Step 6961/9637 - LR:0.0000 - train_loss: 1.582Step 6971/9637 - LR:0.0000 - train_loss: 1.602Step 6981/9637 - LR:0.0000 - train_loss: 1.589Step 6991/9637 - LR:0.0000 - train_loss: 1.600Step 7001/9637 - LR:0.0000 - train_loss: 1.595Step 7011/9637 - LR:0.0000 - train_loss: 1.585Step 7021/9637 - LR:0.0000 - train_loss: 1.590Step 7031/9637 - LR:0.0000 - train_loss: 1.576Step 7041/9637 - LR:0.0000 - train_loss: 1.561Step 7051/9637 - LR:0.0000 - train_loss: 1.600Step 7061/9637 - LR:0.0000 - train_loss: 1.592Step 7071/9637 - LR:0.0000 - train_loss: 1.592Step 7081/9637 - LR:0.0000 - train_loss: 1.583Step 7091/9637 - LR:0.0000 - train_loss: 1.597Step 7101/9637 - LR:0.0000 - train_loss: 1.598Step 7111/9637 - LR:0.0000 - train_loss: 1.584Step 7121/9637 - LR:0.0000 - train_loss: 1.590Step 7131/9637 - LR:0.0000 - train_loss: 1.589Step 7141/9637 - LR:0.0000 - train_loss: 1.582Step 7151/9637 - LR:0.0000 - train_loss: 1.589Step 7161/9637 - LR:0.0000 - train_loss: 1.578Step 7171/9637 - LR:0.0000 - train_loss: 1.579Step 7181/9637 - LR:0.0000 - train_loss: 1.579Step 7191/9637 - LR:0.0000 - train_loss: 1.576Step 7201/9637 - LR:0.0000 - train_loss: 1.562Step 7211/9637 - LR:0.0000 - train_loss: 1.582Step 7221/9637 - LR:0.0000 - train_loss: 1.571Step 7231/9637 - LR:0.0000 - train_loss: 1.581Step 7241/9637 - LR:0.0000 - train_loss: 1.589Step 7251/9637 - LR:0.0000 - train_loss: 1.577Step 7261/9637 - LR:0.0000 - train_loss: 1.578Step 7271/9637 - LR:0.0000 - train_loss: 1.590Step 7281/9637 - LR:0.0000 - train_loss: 1.596Step 7291/9637 - LR:0.0000 - train_loss: 1.593Step 7301/9637 - LR:0.0000 - train_loss: 1.587Step 7311/9637 - LR:0.0000 - train_loss: 1.561Step 7321/9637 - LR:0.0000 - train_loss: 1.591Step 7331/9637 - LR:0.0000 - train_loss: 1.579Step 7341/9637 - LR:0.0000 - train_loss: 1.573Step 7351/9637 - LR:0.0000 - train_loss: 1.566Step 7361/9637 - LR:0.0000 - train_loss: 1.557Step 7371/9637 - LR:0.0000 - train_loss: 1.566Step 7381/9637 - LR:0.0000 - train_loss: 1.544Step 7391/9637 - LR:0.0000 - train_loss: 1.556Step 7401/9637 - LR:0.0000 - train_loss: 1.557Step 7411/9637 - LR:0.0000 - train_loss: 1.585Step 7421/9637 - LR:0.0000 - train_loss: 1.531Step 7431/9637 - LR:0.0000 - train_loss: 1.592Step 7441/9637 - LR:0.0000 - train_loss: 1.548Step 7451/9637 - LR:0.0000 - train_loss: 1.559Step 7461/9637 - LR:0.0000 - train_loss: 1.553Step 7471/9637 - LR:0.0000 - train_loss: 1.545Step 7481/9637 - LR:0.0000 - train_loss: 1.518Step 7491/9637 - LR:0.0000 - train_loss: 1.559Step 7501/9637 - LR:0.0000 - train_loss: 1.561Step 7511/9637 - LR:0.0000 - train_loss: 1.563Step 7521/9637 - LR:0.0000 - train_loss: 1.559Step 7531/9637 - LR:0.0000 - train_loss: 1.560Step 7541/9637 - LR:0.0000 - train_loss: 1.555Step 7551/9637 - LR:0.0000 - train_loss: 1.552Step 7561/9637 - LR:0.0000 - train_loss: 1.554Step 7571/9637 - LR:0.0000 - train_loss: 1.530Step 7581/9637 - LR:0.0000 - train_loss: 1.541Step 7591/9637 - LR:0.0000 - train_loss: 1.540Step 7601/9637 - LR:0.0000 - train_loss: 1.547Step 7611/9637 - LR:0.0000 - train_loss: 1.541Step 7621/9637 - LR:0.0000 - train_loss: 1.533Step 7631/9637 - LR:0.0000 - train_loss: 1.556Step 7641/9637 - LR:0.0000 - train_loss: 1.550Step 7651/9637 - LR:0.0000 - train_loss: 1.546Step 7661/9637 - LR:0.0000 - train_loss: 1.555Step 7671/9637 - LR:0.0000 - train_loss: 1.529Step 7681/9637 - LR:0.0000 - train_loss: 1.533Step 7691/9637 - LR:0.0000 - train_loss: 1.540Step 7701/9637 - LR:0.0000 - train_loss: 1.547Step 7711/9637 - LR:0.0000 - train_loss: 1.546Step 7721/9637 - LR:0.0000 - train_loss: 1.515Step 7731/9637 - LR:0.0000 - train_loss: 1.545Step 7741/9637 - LR:0.0000 - train_loss: 1.547Step 7751/9637 - LR:0.0000 - train_loss: 1.543Step 7761/9637 - LR:0.0000 - train_loss: 1.549Step 7771/9637 - LR:0.0000 - train_loss: 1.530Step 7781/9637 - LR:0.0000 - train_loss: 1.542Step 7791/9637 - LR:0.0000 - train_loss: 1.515Step 7801/9637 - LR:0.0000 - train_loss: 1.523Step 7811/9637 - LR:0.0000 - train_loss: 1.534Step 7821/9637 - LR:0.0000 - train_loss: 1.537Step 7831/9637 - LR:0.0000 - train_loss: 1.530Step 7841/9637 - LR:0.0000 - train_loss: 1.512Step 7851/9637 - LR:0.0000 - train_loss: 1.505Step 7861/9637 - LR:0.0000 - train_loss: 1.527Step 7871/9637 - LR:0.0000 - train_loss: 1.504Step 7881/9637 - LR:0.0000 - train_loss: 1.505Step 7891/9637 - LR:0.0000 - train_loss: 1.512Step 7901/9637 - LR:0.0000 - train_loss: 1.537Step 7911/9637 - LR:0.0000 - train_loss: 1.528Step 7921/9637 - LR:0.0000 - train_loss: 1.519Step 7931/9637 - LR:0.0000 - train_loss: 1.548Step 7941/9637 - LR:0.0000 - train_loss: 1.538Step 7951/9637 - LR:0.0000 - train_loss: 1.531Step 7961/9637 - LR:0.0000 - train_loss: 1.517Step 7971/9637 - LR:0.0000 - train_loss: 1.521Step 7981/9637 - LR:0.0000 - train_loss: 1.521Step 7991/9637 - LR:0.0000 - train_loss: 1.498Step 8001/9637 - LR:0.0000 - train_loss: 1.527Step 8011/9637 - LR:0.0000 - train_loss: 1.510Step 8021/9637 - LR:0.0000 - train_loss: 1.526Step 8031/9637 - LR:0.0000 - train_loss: 1.530Step 8041/9637 - LR:0.0000 - train_loss: 1.505Step 8051/9637 - LR:0.0000 - train_loss: 1.544Step 8061/9637 - LR:0.0000 - train_loss: 1.504Step 8071/9637 - LR:0.0000 - train_loss: 1.501Step 8081/9637 - LR:0.0000 - train_loss: 1.511Step 8091/9637 - LR:0.0000 - train_loss: 1.512Step 8101/9637 - LR:0.0000 - train_loss: 1.533Step 8111/9637 - LR:0.0000 - train_loss: 1.504Step 8121/9637 - LR:0.0000 - train_loss: 1.498Step 8131/9637 - LR:0.0000 - train_loss: 1.523Step 8141/9637 - LR:0.0000 - train_loss: 1.496Step 8151/9637 - LR:0.0000 - train_loss: 1.479Step 8161/9637 - LR:0.0000 - train_loss: 1.491Step 8171/9637 - LR:0.0000 - train_loss: 1.509Step 8181/9637 - LR:0.0000 - train_loss: 1.528Step 8191/9637 - LR:0.0000 - train_loss: 1.475Step 8201/9637 - LR:0.0000 - train_loss: 1.511Step 8211/9637 - LR:0.0000 - train_loss: 1.516Step 8221/9637 - LR:0.0000 - train_loss: 1.528Step 8231/9637 - LR:0.0000 - train_loss: 1.506Step 8241/9637 - LR:0.0000 - train_loss: 1.492Step 8251/9637 - LR:0.0000 - train_loss: 1.497Step 8261/9637 - LR:0.0000 - train_loss: 1.484Step 8271/9637 - LR:0.0000 - train_loss: 1.484Step 8281/9637 - LR:0.0000 - train_loss: 1.507Step 8291/9637 - LR:0.0000 - train_loss: 1.496Step 8301/9637 - LR:0.0000 - train_loss: 1.482Step 8311/9637 - LR:0.0000 - train_loss: 1.503Step 8321/9637 - LR:0.0000 - train_loss: 1.488Step 8331/9637 - LR:0.0000 - train_loss: 1.496Step 8341/9637 - LR:0.0000 - train_loss: 1.497Step 8351/9637 - LR:0.0000 - train_loss: 1.499Step 8361/9637 - LR:0.0000 - train_loss: 1.482Step 8371/9637 - LR:0.0000 - train_loss: 1.480Step 8381/9637 - LR:0.0000 - train_loss: 1.524Step 8391/9637 - LR:0.0000 - train_loss: 1.507Step 8401/9637 - LR:0.0000 - train_loss: 1.444Step 8411/9637 - LR:0.0000 - train_loss: 1.455Step 8421/9637 - LR:0.0000 - train_loss: 1.475Step 8431/9637 - LR:0.0000 - train_loss: 1.476Step 8441/9637 - LR:0.0000 - train_loss: 1.448Step 8451/9637 - LR:0.0000 - train_loss: 1.470Step 8461/9637 - LR:0.0000 - train_loss: 1.471Step 8471/9637 - LR:0.0000 - train_loss: 1.477Step 8481/9637 - LR:0.0000 - train_loss: 1.472Step 8491/9637 - LR:0.0000 - train_loss: 1.481Step 8501/9637 - LR:0.0000 - train_loss: 1.468Step 8511/9637 - LR:0.0000 - train_loss: 1.471Step 8521/9637 - LR:0.0000 - train_loss: 1.495Step 8531/9637 - LR:0.0000 - train_loss: 1.470Step 8541/9637 - LR:0.0000 - train_loss: 1.484Step 8551/9637 - LR:0.0000 - train_loss: 1.484Step 8561/9637 - LR:0.0000 - train_loss: 1.465Step 8571/9637 - LR:0.0000 - train_loss: 1.452Step 8581/9637 - LR:0.0000 - train_loss: 1.456Step 8591/9637 - LR:0.0000 - train_loss: 1.469Step 8601/9637 - LR:0.0000 - train_loss: 1.479Step 8611/9637 - LR:0.0000 - train_loss: 1.476Step 8621/9637 - LR:0.0000 - train_loss: 1.466Step 8631/9637 - LR:0.0000 - train_loss: 1.456Step 8641/9637 - LR:0.0000 - train_loss: 1.464Step 8651/9637 - LR:0.0000 - train_loss: 1.489Step 8661/9637 - LR:0.0000 - train_loss: 1.469Step 8671/9637 - LR:0.0000 - train_loss: 1.468Step 8681/9637 - LR:0.0000 - train_loss: 1.481Step 8691/9637 - LR:0.0000 - train_loss: 1.453Step 8701/9637 - LR:0.0000 - train_loss: 1.465Step 8711/9637 - LR:0.0000 - train_loss: 1.436Step 8721/9637 - LR:0.0000 - train_loss: 1.471Step 8731/9637 - LR:0.0000 - train_loss: 1.451Step 8741/9637 - LR:0.0000 - train_loss: 1.440Step 8751/9637 - LR:0.0000 - train_loss: 1.467Step 8761/9637 - LR:0.0000 - train_loss: 1.470Step 8771/9637 - LR:0.0000 - train_loss: 1.478Step 8781/9637 - LR:0.0000 - train_loss: 1.473Step 8791/9637 - LR:0.0000 - train_loss: 1.456Step 8801/9637 - LR:0.0000 - train_loss: 1.469Step 8811/9637 - LR:0.0000 - train_loss: 1.447Step 8821/9637 - LR:0.0000 - train_loss: 1.444Step 8831/9637 - LR:0.0000 - train_loss: 1.435Step 8841/9637 - LR:0.0000 - train_loss: 1.466Step 8851/9637 - LR:0.0000 - train_loss: 1.472Step 8861/9637 - LR:0.0000 - train_loss: 1.471Step 8871/9637 - LR:0.0000 - train_loss: 1.470Step 8881/9637 - LR:0.0000 - train_loss: 1.452Step 8891/9637 - LR:0.0000 - train_loss: 1.445Step 8901/9637 - LR:0.0000 - train_loss: 1.459Step 8911/9637 - LR:0.0000 - train_loss: 1.446Step 8921/9637 - LR:0.0000 - train_loss: 1.444Step 8931/9637 - LR:0.0000 - train_loss: 1.433Step 8941/9637 - LR:0.0000 - train_loss: 1.454Step 8951/9637 - LR:0.0000 - train_loss: 1.463Step 8961/9637 - LR:0.0000 - train_loss: 1.438Step 8971/9637 - LR:0.0000 - train_loss: 1.451Step 8981/9637 - LR:0.0000 - train_loss: 1.443Step 8991/9637 - LR:0.0000 - train_loss: 1.453Step 9001/9637 - LR:0.0000 - train_loss: 1.447Step 9011/9637 - LR:0.0000 - train_loss: 1.441Step 9021/9637 - LR:0.0000 - train_loss: 1.405Step 9031/9637 - LR:0.0000 - train_loss: 1.439Step 9041/9637 - LR:0.0000 - train_loss: 1.415Step 9051/9637 - LR:0.0000 - train_loss: 1.462Step 9061/9637 - LR:0.0000 - train_loss: 1.463Step 9071/9637 - LR:0.0000 - train_loss: 1.427Step 9081/9637 - LR:0.0000 - train_loss: 1.422Step 9091/9637 - LR:0.0000 - train_loss: 1.427Step 9101/9637 - LR:0.0000 - train_loss: 1.435Step 9111/9637 - LR:0.0000 - train_loss: 1.424Step 9121/9637 - LR:0.0000 - train_loss: 1.435Step 9131/9637 - LR:0.0000 - train_loss: 1.421Step 9141/9637 - LR:0.0000 - train_loss: 1.411Step 9151/9637 - LR:0.0000 - train_loss: 1.434Step 9161/9637 - LR:0.0000 - train_loss: 1.412Step 9171/9637 - LR:0.0000 - train_loss: 1.434Step 9181/9637 - LR:0.0000 - train_loss: 1.405Step 9191/9637 - LR:0.0000 - train_loss: 1.409Step 9201/9637 - LR:0.0000 - train_loss: 1.451Step 9211/9637 - LR:0.0000 - train_loss: 1.426Step 9221/9637 - LR:0.0000 - train_loss: 1.436Step 9231/9637 - LR:0.0000 - train_loss: 1.408Step 9241/9637 - LR:0.0000 - train_loss: 1.441Step 9251/9637 - LR:0.0000 - train_loss: 1.400Step 9261/9637 - LR:0.0000 - train_loss: 1.432Step 9271/9637 - LR:0.0000 - train_loss: 1.412Step 9281/9637 - LR:0.0000 - train_loss: 1.451Step 9291/9637 - LR:0.0000 - train_loss: 1.418Step 9301/9637 - LR:0.0000 - train_loss: 1.419Step 9311/9637 - LR:0.0000 - train_loss: 1.442Step 9321/9637 - LR:0.0000 - train_loss: 1.415Step 9331/9637 - LR:0.0000 - train_loss: 1.428Step 9341/9637 - LR:0.0000 - train_loss: 1.406Step 9351/9637 - LR:0.0000 - train_loss: 1.415Step 9361/9637 - LR:0.0000 - train_loss: 1.386Step 9371/9637 - LR:0.0000 - train_loss: 1.404Step 9381/9637 - LR:0.0000 - train_loss: 1.423Step 9391/9637 - LR:0.0000 - train_loss: 1.424Step 9401/9637 - LR:0.0000 - train_loss: 1.430Step 9411/9637 - LR:0.0000 - train_loss: 1.405Step 9421/9637 - LR:0.0000 - train_loss: 1.429Step 9431/9637 - LR:0.0000 - train_loss: 1.394Step 9441/9637 - LR:0.0000 - train_loss: 1.423Step 9451/9637 - LR:0.0000 - train_loss: 1.419Step 9461/9637 - LR:0.0000 - train_loss: 1.398Step 9471/9637 - LR:0.0000 - train_loss: 1.398Step 9481/9637 - LR:0.0000 - train_loss: 1.418Step 9491/9637 - LR:0.0000 - train_loss: 1.422Step 9501/9637 - LR:0.0000 - train_loss: 1.399Step 9511/9637 - LR:0.0000 - train_loss: 1.402Step 9521/9637 - LR:0.0000 - train_loss: 1.391Step 9531/9637 - LR:0.0000 - train_loss: 1.409Step 9541/9637 - LR:0.0000 - train_loss: 1.401Step 9551/9637 - LR:0.0000 - train_loss: 1.406Step 9561/9637 - LR:0.0000 - train_loss: 1.421Step 9571/9637 - LR:0.0000 - train_loss: 1.394Step 9581/9637 - LR:0.0000 - train_loss: 1.399Step 9591/9637 - LR:0.0000 - train_loss: 1.396Step 9601/9637 - LR:0.0000 - train_loss: 1.375Step 9611/9637 - LR:0.0000 - train_loss: 1.401Step 9621/9637 - LR:0.0000 - train_loss: 1.384Step 9631/9637 - LR:0.0000 - train_loss: 1.408Step 9637/9637 - LR:0.0000 - train_loss: 1.362
exit train!
Once upon a time, Tom is very hungry, but he doesn't have time enough. Something like that?" His mom smiled and said, "No problem! Allow me to come here and break!" Tom was very happy and he went back to playing his game. He went back to playing with his friend, a small lim. They both had lots of fun playing and laughing together. 

I like apple, but Lily loves me. I'll come back, and it will be fun!" Benny wagged his tail and his friends could not stop the important thing. They had a plan to united and help each other animals. From that day, the farmer made sure to put the heavy pile down on the hay. 

Once upon a time, there is a boy named Tom. He is heill,  ⁇  or when mommy's. He is the best friend!" "Nonow, no-chan, until it's not so bad!" Sam barked. He popped his coal nose and his fur was broken. "Oh no, Sam," said his mom. "That's okay. Just make mistakes, but you have to protect yourself. Water can break the bandage easily. It's okay to make mistakes." Tom nodded. He smiled and went to get the bandage. He still liked to be a doctor, but he was happy to be safe with his mom. 

Once upon a time, there is a girl named Lily. One day, she forgot to take a break. She needed to act like the dizzy kitty, but she was scared of it. The little girl's mommy came and asked her to help. Together, they worked together to calm down and then Del feeling better. 

I love the monkey, but he keeps and all his friends and when his time came! He was having fun and he's face. When he finished his dinner, he went back to playing and dancing all day. He was very sleepy, so he decided it was no more. He went inside and lay down on the hanger. He fell asleep and dreamt about being friends." 

Once upon a time, there is a monkey, a big, red pipe in his room and he rewards a big hug. <|endoftext|endoftext| couldn't wait to come back. He put a poke in the cupboard and his eyes opened. He said, "Sorry, Harry. I'm here." The little boy thought about it and said, "It's ok, we'll just have to be careful." He put the stick into the cupboard and all of his friends got a big surprise. Everyone had to look after and look at all the surprises together. In the end, Harry's friends won a game and had a great time. They were both very happy and knew they would come back to the park every day. 

Once upon a time, the sun is dimmed. <||endofoles area. Atsomother can be a class. So, the little boy thought it was funny. He kept trying and hard. Soon, the little boy finished his day and all the other animals shared. He became a great student. 

Once upon a time, the water is dirty. So, instead of stealing the magic thing, a little girl came to the park. She scooped a bite and said, "Yip, what is it?" The little girl said, "It's just the right thing to do. You should be quiet." The mommy smiled and said, "Yes, little girl. We are very serious for losing our team." 

Once upon a time, sophia won the first prize in the competition. <|endofood, he smiled and said, "Straby, I'm, I love you!" He time beeped, "Yes, I like to stay here too, but we can't take a nap!" The little girl smiled and said, "Okay, sweetheart! I'll be quiet." 

Once upon a time, there was a little girl named Lucy. She had a pet cat named Tom. Tom was a small, angry dog, who had come home. "Please, do you want to play?" asked Max. "Yes, you can! But be quiet and don't scare the animals!" said the little girl, who was very friendly. So, Tom started to whistle a happy tune. He whistled louder and louder, but he was just happy that he had been able to play. 

Once upon a time, there was a little brown dog named Spot. <|acety was so relieved, but then he felt a bit guilty. He got frustrated and wanted to have something to say, " same job ever!" He looked at the cat for a moment and said, "O ⁇ !" Finally, the silly dog agreed, and then Digffor filled his bucket with the delicious, soft milk. He grabbed it, and started to walk back home. 

Once upon a time, there was a little boy named Tom. She was about, even when his mom said, "Notop, you should be careful and listen to your parents. Pravy, I don't want to tell you to take a deep breath when we don't want to be punished." So, the naughty fox kept teasing her, and instead of eating the broccoli. By the end of the party, the owner won ⁇ t give in to the same side. The end. 

Once upon a time, there was a big whale. One day, the little boy decided to try something new. He looked around hisMummy, and said he'll only the first boy to come through when he came to her house. The next day, while the little boy had a nap. He fell asleep in his favorite seat. He dreamtud that he ⁇ s searching for something to eat. When the little boy found a dish he aimed food shop. He quickly packed it up and went home. The next day, the little boy returned to the shop with his mom. She had forgotten to bring the dish home. The little boy was so excited he could hardly wait to receive the dish. He ran back home with the plate and proudly showed it to his family. They were so happy for their home of work. The end. 

Once upon a time, there was a big, friendly dog who wanted to help the tidy little boy. He went back to the park and all the other animals, he was looking for something to eat. <|a went around the park, looking at all the tidy sight. She saw a big tree with a big, red apple on the street. She decided it was her favorite snack. She went inside to get her lunch, and they both shared a picnic together. The little boy and the little boy sat together and had a snack of play. They both shared the toys and ran around the park with their friends. They had so much fun! When it was time to go home, they said goodbye and thanked her for the delicious snack. The little boy and his mommy smiled as they walked home together, both feeling happy. 

Once upon a time, he was always cooler and he accidentally teeth. <|endoffoos, he was tired but he was proud of his plan. So, he decided to take a nap. He napped from the day, when he was sleeping, he snooled, never to be alone in a game. He sleeps sound was now a lifeguared and histes felt safe. 

Onceeconian. <|ee smiled as he started to share. He became generous and he taught his friend lots of growing the most of the problem. <|endoftext|endoftext|> Once upon a time, there was a lively little pig. He lived in a little house in the forest. Every day he would go to the playground and play with his friends. One day, he decided to play some hockey. He started to run, jump, and dance. He raced and jumped and scared, until it was time to go home. So, he packed his stick and started running. Suddenly, he heard a sound coming from a nearby bush. It was a big cat! The cat was scared and started to hide. The lion just kept jumping from flower. He was getting bigger and bigger, despite what rustling it. He looked high, but he couldn't see anything. Finally, he arrived home. He put his stick down and closed his eyes carefully. He felt so proud and brave. He knew it was going to be just a bad dream! 

Tim and Lily were playing in the park. Finally, a little girl came up and said, "Look at me, I'mmen. I'm  ⁇ pots me, please." The tough thing said, "No, I will be more helpful, but you have to clean it up first." So, the little girl started to clean her notebook. She took out her crayons and began to draw a picture. Her friends couldn't stay together and all the other kids came over. They were all amazed by the rude figure and its beautiful place was just fun. The hard work was done with Lily's paper and she smiled. She was so proud of herself for making display its image for good care. 

Tom had a coin that he liked very much. He were eager to go and help me when it's only for playing. He'll and hours, he is never!" 

Tim and Mia like to play in the backyard. Ben and his sister started to laugh. They drove and zooming and had fun to play. Finally, Mia was feeling very impatient, so she quickly finished playing and running in her room. In the end, she was able to go back inside for a long afternoon. 

Tom and Mia went to the zoo with Mom and Dad. The moral of this story is that when you push your mommy and are careful, it's important to be careful. It's always important to be careful when you are having a big day!" 

Anna liked to speak to her toys. The bad thing ⁇ s did not listen to Mum, so she took one of them and said, "No problem! Now we can help you. We are a good doctor." 

Lily was playing with her doll in the garden. When she continued her search, she said her grandma musty, "Please, do you want to help me?" The next day, the sun was shining, and the teacher said the fashion snor and finished her lunch. Lily was so happy and surprised that she made it across the meadow. She had had a great time at the party. Whenever she felt tired, she would take off her orange socks and go outside to play. 

Tim likes to talk about politics. One day, a little girl named Sue put the wrapper in her bag and run away. She says, "Goodbye,!" And then they both fell asleep, with a smile on their faces. 

Sophia never eats breakfast. <|endof-and, "That's what? You're a little boy." The little boy smiled and said, "I'm not?" His mom said, "Sneigh, but, not thinking. You might get lost if you can't." So, the little boy tried his best to learn to think of a way to be brave and have fun. He then went back to his house and gave them a big hug. 

Lucy tell a weird story. <|endofemen, however his friends had the best time, them would always be good friends. 

Lucy and Lily are playing computer games. So, Tom appeared. He said, "That's what ⁇ s fun! I'me and catch treats." The little boy smiled and said, "I'm sorry, kids. I'm sorry, we won't do it again." The next day, Tom went back to Amy's house. He was happy he was safe. 

checkpoint saved!
Executing command >>>> 
   srun --pty -c 10 -p makkapakka --export=ALL --gpus=5  ./run.sh

